<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice</title>
  <meta name="description" content="&#39;Website for ICCV 2025 Tutorial &quot;Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice&quot;&#39;">
  

  <link rel="stylesheet" href="/iccv2025-tutorial/assets/main.css">
  <link rel="canonical" href="https://low-dim-models-tutorials.github.io/iccv2025-tutorial/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice" href="https://low-dim-models-tutorials.github.io/iccv2025-tutorial/feed.xml">

  
<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- MathJax -->
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>



<link
  href="https://fonts.googleapis.com"
  rel="preconnect"
  />
<link
  href="https://fonts.gstatic.com"
  rel="preconnect"
  crossorigin="anonymous"
  />
<script
  src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
  type="text/javascript"
  ></script>
<script type="text/javascript">
  WebFont.load({
    google: {
      families: [
        "Lato:300,300italic,400,400italic,700,700italic",
        "Open Sans:300,300italic,400,400italic,700,700italic",
      ],
    },
  });
</script>

<meta content="https://low-dim-models-tutorials.github.io/iccv2025-tutorial/assets/card.png" property="og:image">
<meta content="https://low-dim-models-tutorials.github.io/iccv2025-tutorial/assets/card.png" property="twitter:image">
<!-- <meta content="summary_large_image" name="twitter:card"> -->


  
  <meta property="og:title" content="Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice">
  <meta property="og:site_name" content="Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice">
  <meta property="og:url" content="https://low-dim-models-tutorials.github.io/iccv2025-tutorial/">
  <meta property="og:description" content="&#39;Website for ICCV 2025 Tutorial &quot;Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice&quot;&#39;">
  
  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:title" content="Learning Deep Low-Dimensional Models from High-Dimensional Data: Fr...">
  <meta name="twitter:description" content="&#39;Website for ICCV 2025 Tutorial &quot;Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice&quot;&#39;">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/iccv2025-tutorial/">
    </a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/iccv2025-tutorial/#overview">Overview</a>
      
        
        <a class="page-link" href="/iccv2025-tutorial/#speakers">Speakers</a>
      
        
        <a class="page-link" href="/iccv2025-tutorial/#panelists">Panelists</a>
      
        
        <a class="page-link" href="/iccv2025-tutorial/#schedule">Schedule</a>
      
        
        <a class="page-link" href="/iccv2025-tutorial/#materials">Materials</a>
      
    </nav>

  </div>

</header>

    <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
  <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>

</svg>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
            <center>
<h1>
  
  
    
    Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory
    to Practice
  
  
  
</h1>
    
<h3 style="color: #44434d">
  
  
    <b>ICCV 2025 Tutorial</b>
  
  
</h3>
    
<h3 style="color: #44434d">
  
  
    
    <b>Date:</b> TBD (full day tutorial)
  
  
  
</h3>
    
<h3 style="color: #44434d">
  
  
    <b>Location:</b> TBD
  
  
</h3>
    

  <div class="svg-container">
    <!-- Embedding SVG image with <img> tag -->
    <img src="/iccv2025-tutorial/assets/representation.svg" alt="Representation Learning SVG Image" style="display: block; height: 200px" />
  </div>
</center>
<h2 id="overview">
  
  
    <a href="#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a> Overview
  
  
</h2>
    

<p>Over the past decade, the advent of machine learning and large-scale computing
has immeasurably changed the ways we process, interpret, and predict with data
in imaging and computer vision. The “traditional” approach to algorithm
design, based around parametric models for specific structures of signals and
measurements—say sparse and low-rank models—and the associated optimization
toolkit, is now significantly enriched with data-driven learning-based
techniques, where large-scale networks are pre-trained and then adapted to a
variety of specific tasks. Nevertheless, the successes of both modern
data-driven and classic model-based paradigms rely crucially on correctly
identifying the low-dimensional structures present in real-world data, to the
extent that we see the roles of learning and compression of data processing
algorithms—whether explicit or implicit, as with deep networks—as
inextricably linked.</p>

<p>As such, this tutorial provides a timely tutorial that
uniquely bridges low-dimensional models with deep learning in imaging and
vision. This tutorial will show how:</p>

<ol>
  <li>Low-dimensional models and principles provide a valuable lens for
formulating problems and understanding the behavior of modern deep models in
imaging and computer vision; and how</li>
  <li>Ideas from low-dimensional models can provide valuable guidance for
designing new parameter efficient, robust, and interpretable deep learning
models for computer vision problems in practice.</li>
</ol>

<p>We will begin by introducing
fundamental low-dimensional models (e.g., basic sparse and low-rank models)
with motivating computer vision applications.
Based on these developments, we
will discuss strong conceptual, algorithmic, and theoretical connections
between low-dimensional structures and deep models, providing new perspectives
to understand state-of-the-art deep models in terms of learned representations
and generative models.
Finally, we will demonstrate that these
connections can lead to new principles for designing deep networks and learning
low-dimensional structures in computer vision, with both clear interpretability
and practical benefits.
We will conclude with a <strong>panel discussion</strong> with expert researchers from academia
and industry on what role low-dimensional models can and should play in our
current age of generative AI and foundation models for computer
vision.</p>
<h2 id="speakers">
  
  
    <a href="#speakers" class="anchor-heading" aria-labelledby="speakers"><svg viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a> Speakers
  
  
</h2>
    

<div style="clear: both; display: flex; flex-wrap: wrap; justify-content:
  space-evenly; ">



<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/sdb.jpg" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://sdbuchanan.com/">Sam Buchanan</a></center>
      
    
  
  
</h3>
    
    
    <center><p>TTIC</p></center>
    
    

  </div>
</div>


<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/ma.jpeg" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a></center>
      
    
  
  
</h3>
    
    
    <center><p>UC Berkeley<br />HKU IDS</p></center>
    
    

  </div>
</div>


<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/qq.jpeg" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://qingqu.engin.umich.edu/">Qing Qu</a></center>
      
    
  
  
</h3>
    
    
    <center><p>UMichigan</p></center>
    
    

  </div>
</div>


<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/shen-liyue.jpeg" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://liyueshen.engin.umich.edu/">Liyue Shen</a></center>
      
    
  
  
</h3>
    
    
    <center><p>UMichigan</p></center>
    
    

  </div>
</div>


<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/aw.png" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://vita-group.github.io/">Atlas Wang</a></center>
      
    
  
  
</h3>
    
    
    <center><p>UT Austin</p></center>
    
    

  </div>
</div>


<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/zz.jpeg" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://zhihuizhu.github.io">Zhihui Zhu</a></center>
      
    
  
  
</h3>
    
    
    <center><p>Ohio State</p></center>
    
    

  </div>
</div>



</div>
<h2 id="panelists">
  
  
    <a href="#panelists" class="anchor-heading" aria-labelledby="panelists"><svg viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a> Panelists
  
  
</h2>
    

<div style="clear: both; display: flex; flex-wrap: wrap; justify-content:
  space-evenly; ">



<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/bi.png" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://sites.google.com/view/berivanisik/">Berivan Isik</a></center>
      
    
  
  
</h3>
    
    
    <center><p>Google</p></center>
    
    

  </div>
</div>


<div class="organizer" style="width: 102px;">
  
  <img class="organizer-image" src="/iccv2025-tutorial/assets/images/vp.png" alt="" />
  
  <div>
<h3 class="organizer-name">
  
  
    
      
      <center><a href="https://www.linkedin.com/in/vladimir-pavlovic-a5528412/">Vladimir Pavlovic</a></center>
      
    
  
  
</h3>
    
    
    <center><p>NSF &amp; Rutgers University</p></center>
    
    

  </div>
</div>



</div>
<h2 id="schedule">
  
  
    <a href="#schedule" class="anchor-heading" aria-labelledby="schedule"><svg viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a> Schedule
  
  
</h2>
    

<p>The tutorial will take place at <strong>ICCV 2025</strong>.</p>

<table>
<colgroup>
<col width="69%" />
<col width="17%" />
<col width="14%" />
</colgroup>
<thead>
<tr>
<th>Lecture</th>
<th>Speaker</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td class="title" colspan="3">
<strong>Session I:</strong> Introduction of Basic Low-dimensional Models in Vision
</td>
</tr>
<tr>
<td>
Introduction of Basic Low-dimensional Models
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
The first part will introduce fundamental properties and results for sensing, processing, analyzing, and learning low-dimensional structures from high-dimensional data. We will first discuss classical low-dimensional models, such as sparse coding and low-rank matrix sensing, and motivate these models by applications in computer vision. Based on convex relaxation, we will characterize the conditions, in terms of sample/data complexity, under which the learning problems of recovering such low-dimensional structures become tractable and can be solved efficiently, with guaranteed correctness or accuracy.
</p>
</div>
</td>
<td>
  <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
</td>
<td>
9:00-9:45 am
</td>
</tr>
<tr>
<td>
Introduction of Low-dimensional Models in Deep Learning
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
Next, we delve into the core principles of low-dimensional models within various neural network architectures. Specifically, we will introduce unrolled optimization as a design principle for interpretable deep networks. As a simple special case, we will examine several unrolled optimization algorithms for sparse coding, and show that they exhibit striking similarities to current deep network architectures. These unrolled networks are white-box and interpretable ab initio.
</p>
</div>
</td>
<td>
  <a href="https://vita-group.github.io/">Atlas Wang</a>
</td>
<td>
9:45-10:30 am
</td>
</tr>
<tr>
<td class="title" colspan="3">
<strong>Session II:</strong> Understanding Low-Dimensional Structures in Representation Learning
</td>
</tr>
<tr>
<td>
Understanding Low-Dimensional Representation in Last-layer
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
The session focuses on the strong conceptual connections between low-dimensional structures and deep models in terms of learned representation. We start with the introduction of an intriguing Neural Collapse phenomenon in the last-layer representation and its universality in deep network, and lay out the mathematical foundations of understanding its cause by studying its optimization landscapes. We then generalize and explain this phenomenon and its implications under data imbalanceness. Furthermore, we demonstrate the practical algorithmic implications of Neural Collapse on training deep neural networks.
</p>
</div>
</td>
<td>
  <a href="https://cse.osu.edu/people/zhu.3440">Zhihui Zhu</a>
</td>
<td>
10:45-11:30 am
</td>
</tr>
<tr>
<td>
Understanding Low-Dimensional Representation in Intermediate Layer
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
Second, we show how low-dimensional structures also emerge in intermediate layers of learned deep neural networks. Specifically, we will characterize the progressive feature compression and discrimination from shallow to deep layers. Such a phenomenon is strongly correlated with the low-dimensional structure in learning dynamics, where the evolution of gradient descent only affects a minimal portion of singular vector spaces across all weight matrices.
</p>
</div>
</td>
<td>
  <a href="https://qingqu.engin.umich.edu/">Qing Qu</a>
</td>
<td>
11:30 am-12:00 pm
</td>
</tr>
<tr>
<td class="title" colspan="3">
<strong>Session III:</strong> Understanding Low-Dimensional Structures in Diffusion Generative Models
</td>
</tr>
<tr>
<td>
Low-Dimensional Models for Understanding Generalization in Diffusion Models
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
We will establish a thorough understanding of the generalization of diffusion models by investigating when and why they are capable of learning the low-dimensional structure of target distributions. We will study the sample complexity to learn these low-dimensional distributions and how they scale with the intrinsic dimensionality of data. We will also show how diffusion models transit from memorization to generalization in terms of model capacity and data size.
</p>
</div>
</td>
<td>
  <a href="https://qingqu.engin.umich.edu/">Qing Qu</a>
</td>
<td>
1:15-2:00 pm
</td>
</tr>
<tr>
<td>
Exploring Low-Dimensional Structures for Controlling Diffusion Models
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
We will enhance diffusion models for scientific imaging by improving their efficiency, robustness, and controllability in solving general inverse problems. Our approach focuses on latent-space and patch-based models for efficiency, novel techniques for data consistency in reverse sampling—especially for 3D imaging—and controllable sampling to enforce desired constraints while maintaining sample quality.
</p>
</div>
</td>
<td>
  <a href="https://liyueshen.engin.umich.edu/">Liyue Shen</a>
</td>
<td>
2:00-2:45 pm
</td>
</tr>
<tr>
<td class="title" colspan="3">
<strong>Session IV:</strong> Designing Deep Networks for Pursuing Low-Dimensional Structures
</td>
</tr>
<tr>
<td>
ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
We focus on learning data distribution and transforming it into a linear discriminative representation (LDR). Using information-theoretic and statistical principles, we design a loss function called coding rate reduction, which is optimized at such a representation. By unrolling gradient ascent on this loss, we construct ReduNet, a deep network where each operator has a mathematically precise, interpretable role in transforming data toward an LDR. Notably, ReduNet can be built layer-wise through forward propagation, eliminating the need for back-propagation.
</p>
</div>
</td>
<td>
  <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
</td>
<td>
3:00-3:45 pm
</td>
</tr>
<tr>
<td>
White-Box Transformers via Sparse Rate Reduction
<br />
<a class="abstract btn btn-sm z-depth-0" role="button" style="color:#959396;">(Lecture Abstract)</a>
<br />
<div class="abstract hidden">
<p>
Finally, we introduce sparse linear discriminative representations by combining sparse coding with rate reduction, optimizing an objective called sparse rate reduction. This leads to CRATE, a deep network derived by unrolling the optimization and parameterizing feature distributions layer-wise. CRATE's operators are mathematically interpretable, with each layer corresponding to an optimization step, making it a white-box model. Despite its distinct design from ReduNet, both share a common goal, highlighting the flexibility of unrolled optimization. Interestingly, CRATE closely resembles transformer architectures, suggesting that such interpretability advances could enhance our understanding of modern deep networks.
</p>
</div>
</td>
<td>
  <a href="https://sdbuchanan.com">Sam Buchanan</a>
</td>
<td>
3:45-4:30 pm
</td>
</tr>
<tr>
<td class="title" colspan="2">
<strong>Session V:</strong> Panel Discussion (led by Liyue Shen)
</td>
<td>
4:45-5:45 pm
</td>
</tr>
</tbody>
</table>
<h2 id="materials">
  
  
    <a href="#materials" class="anchor-heading" aria-labelledby="materials"><svg viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a> Materials
  
  
</h2>
    

<p>Materials for the tutorial will be made available closer to the conference date.</p>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
       

<!-- <div id="icon-container"> -->
<!--   <div> -->
<!--     <a href=mailto: style="text-decoration: none"> -->
<!--       <img id="icon-container-item" src="/assets/icons/gmail.svg" alt="Mail icon" /> -->
<!--     </a> -->
<!--   </div> -->
<!--   <div> -->
<!--     <a href=https://scholar.google.com/citations?user= style="text-decoration: none"> -->
<!--       <img id="icon-container-item" src="/assets/icons/scholar.svg" alt="Google Scholar icon" /> -->
<!--     </a> -->
<!--   </div> -->
<!---->
<!--   <div> -->
<!--     <a href=https://twitter.com/ -->
<!--        style="text-decoration: none"> -->
<!--       <img id="icon-container-item" src="/assets/icons/twitter.svg" alt="Twitter icon" /> -->
<!--     </a> -->
<!--   </div> -->
<!---->
<!--   <div> -->
<!--     <a href=https://github.com/ -->
<!--        style="text-decoration: none"> -->
<!--       <img id="icon-container-item" src="/assets/icons/github.svg" alt="GitHub icon" /> -->
<!--     </a> -->
<!--   </div> -->
<!-- </div> -->

<div>
  © 2025 <a href="https://sdbuchanan.com">Sam Buchanan</a>,
  <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a>,
  <a href="https://qingqu.engin.umich.edu/">Qing Qu</a>,
  <a href="https://liyueshen.engin.umich.edu/">Liyue Shen</a>,
  <a href="https://vita-group.github.io/">Atlas Wang</a>,
  <a href="https://cse.osu.edu/people/zhu.3440">Zhihui Zhu</a>
</div>

    </p>

  </div>

</footer>


  </body>

  <!-- Load Common JS -->
<script src="/iccv2025-tutorial/assets/js/common.js"></script>


</html>
